# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fWlsQGzeK8r4x0gcspQnKBZeg8Zx493D
"""

import numpy as np
import pandas as pd
import joblib
import tkinter

crime = pd.read_csv('Crime Prediction in Chicago_Dataset.csv')
print(len(crime))

crime.head()
crime.isna().sum()

# prompt: generate a code that removes duplicate values

crime.drop_duplicates(inplace=True)

crime.columns

crime= crime[['Date', 'Block', 'IUCR', 'Primary Type',
       'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat',
       'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate',
       'Y Coordinate', 'Year','Latitude', 'Longitude']]

crime.head()

# Handling null Values

crime['Longitude'] = crime['Longitude'].fillna(crime['Longitude'].mean())
crime['Latitude'] = crime['Latitude'].fillna(crime['Latitude'].mean())
crime['X Coordinate'] = crime['X Coordinate'].fillna(crime['X Coordinate'].mean())
crime['Y Coordinate'] = crime['Y Coordinate'].fillna(crime['Y Coordinate'].mean())

mode_value = crime['Location Description'].mode()[0]

# Fill NaN values with the mode
crime['Location Description'] = crime['Location Description'].fillna(mode_value)

crime.dropna(inplace=True)

crime.isna().sum()

crime.head()



def detect_outliers_iqr(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    return outliers

# Select numerical columns for outlier detection
numerical_cols = crime.select_dtypes(include=np.number).columns

# Detect outliers in each numerical column
outliers_count = {}
for col in numerical_cols:
    outliers = detect_outliers_iqr(crime[col])
    outliers_count[col] = len(outliers)

# Print the number of outliers in each column
for col, count in outliers_count.items():
    print(f"Number of outliers in {col}: {count}")

# Calculate the total number of outliers
total_outliers = sum(outliers_count.values())
print(f"\nTotal number of outliers across all numerical columns: {total_outliers}")

# prompt: remove outliers from data

# Remove outliers from the DataFrame
for col in numerical_cols:
    Q1 = crime[col].quantile(0.25)
    Q3 = crime[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    crime = crime[(crime[col] >= lower_bound) & (crime[col] <= upper_bound)]

# Print the shape of the DataFrame after removing outliers
print("\nShape of DataFrame after outlier removal:", crime.shape)

crime.isna().sum()

#Handling block and date columns

# Convert 'Date' column to datetime objects
crime['Date'] = pd.to_datetime(crime['Date'])

# Extract day, day of the week, month, and hour
crime['Day'] = crime['Date'].dt.day
crime['DayOfWeek'] = crime['Date'].dt.dayofweek # Monday=0, Sunday=6
crime['Month'] = crime['Date'].dt.month
crime['Hour'] = crime['Date'].dt.hour
crime['Minute'] = crime['Date'].dt.minute

# Combine hour and minute into a single integer
crime['HourMinute'] = crime['Hour'] * 100 + crime['Minute']

# Drop the original 'Date', 'Minute' columns
crime = crime.drop(['Date', 'Minute'], axis=1)

# prompt: convert block column into Street number and street, the numerical values are in street number while strings are in street

# Splitting the 'Block' column
crime[['Street Number', 'Street']] = crime['Block'].str.split('00', n=1, expand=True)

# Removing unnecessary characters from 'Street Number'
crime['Street Number'] = crime['Street Number'].str.replace(r'\D+', '', regex=True)

# Converting 'Street Number' to numeric, coercing errors to NaN
crime['Street Number'] = pd.to_numeric(crime['Street Number'], errors='coerce')

# Filling NaN values in 'Street Number' with 0 (you might want to adjust this based on your needs)
crime['Street Number'] = crime['Street Number'].fillna(0)

# Removing leading/trailing whitespaces from 'Street'
# crime['Street'] = crime['Street'].str.strip()

crime.head()

crime.drop(['Block'], axis=1, inplace=True)
crime.drop(['Street'], axis=1, inplace=True)

crime.head()

crime.drop(['Description'], axis=1, inplace=True)

# Label encoding Loaction Description and Primary Type

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
crime['Location Description'] = le.fit_transform(crime['Location Description'])
crime['Primary Type'] = le.fit_transform(crime['Primary Type'])

crime.head()

# prompt: if i want convert arrest coulmn to 0 or 1

# Convert 'Arrest' column to 0 or 1
crime['Arrest'] = crime['Arrest'].astype(int)
crime['Domestic'] = crime['Domestic'].astype(int)

crime.head()

crime.head()

# prompt: show me the data type of IUCR and FBI Code

print(crime['IUCR'].dtype)
print(crime['FBI Code'].dtype)

# Label encoding IUCR and FBI Code

le = LabelEncoder()
crime['IUCR'] = le.fit_transform(crime['IUCR'])
crime['FBI Code'] = le.fit_transform(crime['FBI Code'])

# Regularisation for X and Y Coordinate and Longitude and Latitude and Ward and Beat and District using z score

from scipy.stats import zscore

crime['X Coordinate'] = zscore(crime['X Coordinate'])
crime['Y Coordinate'] = zscore(crime['Y Coordinate'])
crime['Longitude'] = zscore(crime['Longitude'])
crime['Latitude'] = zscore(crime['Latitude'])
crime['Ward'] = zscore(crime['Ward'])
crime['Beat'] = zscore(crime['Beat'])
crime['District'] = zscore(crime['District'])



# prompt: do a correlation matrix

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'crime' DataFrame is already prepared as in your previous code

# Calculate the correlation matrix
correlation_matrix = crime.corr()

# Plotting the correlation matrix using a heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Crime Data')
plt.show()

# prompt: apply mutual information feature selection method
"""
from sklearn.feature_selection import mutual_info_classif

# Separate features (X) and target variable (y)
X = crime.drop('Arrest', axis=1)  # Assuming 'Arrest' is your target variable
y = crime['Arrest']

# Calculate mutual information
mutual_info = mutual_info_classif(X, y)

# Create a DataFrame to store feature importances
feature_importances = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mutual_info})

# Sort the features by mutual information in descending order
feature_importances = feature_importances.sort_values('Mutual Information', ascending=False)

# Print the feature importances
feature_importances

"""

crime.head()

crime.drop(['Beat', 'District', 'Y Coordinate', 'Latitude', 'DayOfWeek','Street Number'], axis=1, inplace=True)

# prompt: display how many rows

print(len(crime))

from sklearn.model_selection import train_test_split

# Assuming 'data' is your DataFrame and 'target' is the target column

X = crime.drop(columns='Arrest')
y = crime['Arrest']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression

# Initialize Logistic Regression model
logistic_model = LogisticRegression(class_weight='balanced')

from sklearn.model_selection import cross_val_score

#scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')


# Train the model
logistic_model.fit(X_train, y_train)
#print(f"Average ROC-AUC: {scores.mean()}")



# Predict probabilities
y_pred_probs = logistic_model.predict_proba(X_test)[:, 1]  # Probabilities for class 1 (crime)
threshold = 0.5  # Example threshold
y_pred = (y_pred_probs >= threshold).astype(int)
# Predict binary classes (0 or 1)

print("Predicted Classes:", y_pred)
print("Predicted Probabilities:", y_pred_probs)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Accuracy
accuracy = accuracy_score(y_test, y_pred)

# Precision
precision = precision_score(y_test, y_pred)

# Recall
recall = recall_score(y_test, y_pred)

# F1-Score
f1 = f1_score(y_test, y_pred)

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_pred_probs)

# Print metrics
print(f"LogisticRegression Accuracy: {accuracy}")
print(f"LogisticRegression Precision: {precision}")
print(f"LogisticRegression Recall: {recall}")
print(f"LogisticRegression F1 Score: {f1}")
print(f"LogisticRegression ROC-AUC: {roc_auc}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Use seaborn to plot the confusion matrix
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

#!pip install --upgrade pandas==2.0.3 # Downgrade pandas to a compatible version
#!pip install --upgrade xgboost # Ensure xgboost is also updated

import xgboost as xgb

# Initialize the XGBoost classifier
xgb_model = xgb.XGBClassifier(enable_categorical=True)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred = xgb_model.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Accuracy
accuracy = accuracy_score(y_test, y_pred)

# Precision
precision = precision_score(y_test, y_pred)

# Recall
recall = recall_score(y_test, y_pred)

# F1-Score
f1 = f1_score(y_test, y_pred)

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_pred_probs)

# Print metrics
print(f"XGBoost Accuracy: {accuracy}")
print(f"XGBoost Precision: {precision}")
print(f"XGBoost Recall: {recall}")
print(f"XGBoost F1 Score: {f1}")
print(f"XGBoost ROC-AUC: {roc_auc}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("XGBoost Confusion Matrix:")
print(cm)


# Use seaborn to plot the confusion matrix
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# prompt: apply KNN algorithm on the model

from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier
knn_model = KNeighborsClassifier(n_neighbors=5) # You can adjust the number of neighbors

# Train the model
knn_model.fit(X_train, y_train)

# Make predictions
y_pred_knn = knn_model.predict(X_test)

# Evaluate the model
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f"KNN Accuracy: {accuracy_knn}")

# Other metrics
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)

print(f"KNN Precision: {precision_knn}")
print(f"KNN Recall: {recall_knn}")
print(f"KNN F1 Score: {f1_knn}")

# Confusion Matrix
cm_knn = confusion_matrix(y_test, y_pred_knn)
print("KNN Confusion Matrix:")

# Use seaborn to plot the confusion matrix
sns.heatmap(cm_knn, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# prompt: apply decision tree algorithm

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

# Initialize the Decision Tree classifier
dt_model = DecisionTreeClassifier(random_state=42, max_depth = 5) # You can adjust hyperparameters

# Train the model
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy: {accuracy_dt}")

# Other metrics
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)

print(f"Decision Tree Precision: {precision_dt}")
print(f"Decision Tree Recall: {recall_dt}")
print(f"Decision Tree F1 Score: {f1_dt}")

# Confusion Matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
print("Decision Tree Confusion Matrix:")
print(cm_dt)

# Use seaborn to plot the confusion matrix
sns.heatmap(cm_dt, annot=True, fmt="d")
plt.title('Decision Tree Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# prompt: apply svm algorithm on model

from sklearn.svm import SVC

# Initialize the SVM classifier
svm_model = SVC(kernel='linear', probability=True, random_state=42,max_iter=1000) # You can change the kernel

# Train the model
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test)
y_pred_svm_probs = svm_model.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"SVM Accuracy: {accuracy_svm}")

# Other metrics
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
roc_auc_svm = roc_auc_score(y_test, y_pred_svm_probs)

print(f"SVM Precision: {precision_svm}")
print(f"SVM Recall: {recall_svm}")
print(f"SVM F1 Score: {f1_svm}")
print(f"SVM ROC-AUC: {roc_auc_svm}")


# Confusion Matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("SVM Confusion Matrix:")
print(cm_svm)

# Use seaborn to plot the confusion matrix
sns.heatmap(cm_svm, annot=True, fmt="d")
plt.title('SVM Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# prompt: apply lightgbm algorithm for model

#!pip install lightgbm

import lightgbm as lgb

# Initialize the LightGBM classifier
lgb_model = lgb.LGBMClassifier()

# Train the model
lgb_model.fit(X_train, y_train)

# Make predictions
y_pred_lgb = lgb_model.predict(X_test)
y_pred_lgb_probs = lgb_model.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy_lgb = accuracy_score(y_test, y_pred_lgb)
print(f"LightGBM Accuracy: {accuracy_lgb}")

# Other metrics
precision_lgb = precision_score(y_test, y_pred_lgb)
recall_lgb = recall_score(y_test, y_pred_lgb)
f1_lgb = f1_score(y_test, y_pred_lgb)
roc_auc_lgb = roc_auc_score(y_test, y_pred_lgb_probs)

print(f"LightGBM Precision: {precision_lgb}")
print(f"LightGBM Recall: {recall_lgb}")
print(f"LightGBM F1 Score: {f1_lgb}")
print(f"LightGBM ROC-AUC: {roc_auc_lgb}")

# Confusion Matrix
cm_lgb = confusion_matrix(y_test, y_pred_lgb)
print("LightGBM Confusion Matrix:")
print(cm_lgb)

# Use seaborn to plot the confusion matrix
sns.heatmap(cm_lgb, annot=True, fmt="d")
plt.title('LightGBM Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# prompt: apply catboost for model

#pip install catboost

from catboost import CatBoostClassifier

# Initialize the CatBoost classifier
catboost_model = CatBoostClassifier(verbose=0) # Set verbose to 0 to suppress training output

# Train the model
catboost_model.fit(X_train, y_train)

# Make predictions
y_pred_catboost = catboost_model.predict(X_test)
y_pred_catboost_probs = catboost_model.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy_catboost = accuracy_score(y_test, y_pred_catboost)
print(f"CatBoost Accuracy: {accuracy_catboost}")

# Other metrics
precision_catboost = precision_score(y_test, y_pred_catboost)
recall_catboost = recall_score(y_test, y_pred_catboost)
f1_catboost = f1_score(y_test, y_pred_catboost)
roc_auc_catboost = roc_auc_score(y_test, y_pred_catboost_probs)

print(f"CatBoost Precision: {precision_catboost}")
print(f"CatBoost Recall: {recall_catboost}")
print(f"CatBoost F1 Score: {f1_catboost}")
print(f"CatBoost ROC-AUC: {roc_auc_catboost}")

# Confusion Matrix
cm_catboost = confusion_matrix(y_test, y_pred_catboost)
print("CatBoost Confusion Matrix:")
print(cm_catboost)

# Use seaborn to plot the confusion matrix
sns.heatmap(cm_catboost, annot=True, fmt="d")
plt.title('CatBoost Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()
# Load the preprocessing objects and models
import joblib
import pandas as pd
import numpy as np
import tkinter as tk
from tkinter import ttk, messagebox

joblib.dump(le, 'label_encoder.joblib') 
joblib.dump(logistic_model, 'logistic_model.joblib') 
joblib.dump(xgb_model, 'xgb_model.joblib') 
joblib.dump(knn_model, 'knn_model.joblib') 
joblib.dump(dt_model, 'decision_tree_model.joblib') 
joblib.dump(svm_model, 'svm_model.joblib') 
joblib.dump(lgb_model, 'lightgbm_model.joblib') 
joblib.dump(catboost_model, 'catboost_model.joblib')
# Load the preprocessing objects and models
try:
    le = joblib.load("label_encoder.joblib")  # Load label encoder
    

    # Load trained models
    logistic_model = joblib.load("logistic_model.joblib")
    xgb_model = joblib.load("xgb_model.joblib")
    knn_model = joblib.load("knn_model.joblib")
    dt_model = joblib.load("decision_tree_model.joblib")
    svm_model = joblib.load("svm_model.joblib")
    lgb_model = joblib.load("lightgbm_model.joblib")
    catboost_model = joblib.load("catboost_model.joblib")
except Exception as e:
    messagebox.showerror("Error", f"Failed to load models or preprocessing objects: {e}")

# Function to validate user inputs
def validate_inputs(inputs):
    try:
        inputs['Domestic'] = int(inputs['Domestic'])
        inputs['Ward'] = float(inputs['Ward'])
        inputs['X Coordinate'] = float(inputs['X Coordinate'])
        inputs['Longitude'] = float(inputs['Longitude'])
        inputs['Hour'] = int(inputs['Hour'])
        inputs['Month'] = int(inputs['Month'])
        inputs['HourMinute'] = int(inputs['HourMinute'])
        inputs['Year'] = int(inputs['Year'])
        inputs['Community Area'] = pd.to_numeric(inputs['Community Area'], errors='coerce', downcast='integer')
        inputs['FBI Code'] = pd.to_numeric(inputs['FBI Code'], errors='coerce', downcast='integer')
        inputs['Day'] = pd.to_numeric(inputs['Day'], errors='coerce', downcast='integer')
        return inputs
     # Convert Community Area, FBI Code, and Day to integers
        
    except ValueError as ve:
        messagebox.showerror("Error", "Invalid input! Please ensure all fields are filled correctly.")
        raise ve

# Function to predict arrest
def predict_arrest():
    try:
        # Collect inputs from the user
        inputs = {
            'Primary Type': entry_primary_type.get(),
            'Location Description': entry_location_description.get(),
            'IUCR': entry_IUCR.get(),
            'Domestic': entry_domestic.get(), 
            'Ward': entry_ward.get(),
            'X Coordinate': entry_x_coord.get(),
            'Longitude': entry_longitude.get(),
            'Hour': entry_hour.get(),
            'Month': entry_month.get(),
            'HourMinute': entry_hour_minute.get(),
            'FBI Code': entry_FBI_Code.get(),
            'Year': entry_year.get(),
            'Day': entry_day.get(),
            'Community Area': entry_Community_Area.get()
        }

        # Validate and format inputs
        inputs = validate_inputs(inputs)
        input_data = pd.DataFrame([inputs])

        # Encode categorical features
        for col in ['Primary Type', 'Location Description', 'IUCR']:
            if input_data[col].values[0] not in le.classes_:
                le.classes_ = np.append(le.classes_, 'Unknown')
                input_data[col] = 'Unknown'
            input_data[col] = le.transform([input_data[col].values[0]])[0]

        # Ensure the input_data columns are in the same order as X_train
        input_data = input_data[X.columns]  # X.columns ensures the same order of columns

        # Select the model
        selected_model = combo_model.get()
        if not selected_model:
            messagebox.showerror("Error", "Please select a model")
            return

        # Map model selection to the actual model
        models = {
            'Logistic Regression': logistic_model,
            'XGBoost': xgb_model,
            'KNN': knn_model,
            'Decision Tree': dt_model,
            'SVM': svm_model,
            'LightGBM': lgb_model,
            'CatBoost': catboost_model
        }
        model_name = selected_model.split(" (")[0]
        model = models.get(model_name)

        if not model:
            messagebox.showerror("Error", "Selected model not found.")
            return

        # Make prediction
        if hasattr(model, "predict_proba"):
            probabilities = model.predict_proba(input_data)[:, 1]  # Probability of class 1
        else:
            probabilities = model.decision_function(input_data)
            probabilities = (probabilities - probabilities.min()) / (probabilities.max() - probabilities.min())  # Normalize

        prediction = (probabilities[0] >= 0.5).astype(int)  # Threshold for binary classification
        predicted_class = 'Yes' if prediction else 'No'

        # Calculate precision for the selected model
        y_pred = model.predict(X_test)
        precision = precision_score(y_test, y_pred)

        # Display results
        messagebox.showinfo(
            "Prediction",
            f"Predicted Arrest: {predicted_class}\n"
            f"Probability of Arrest: {probabilities[0]:.2f}\n"
            f"Model Precision: {precision:.2f}"
        )

    except Exception as e:
        messagebox.showerror("Error", f"An error occurred: {e}")


# GUI setup
root = tk.Tk()
root.title("Crime Arrest Prediction")

# Feature Inputs
features_frame = ttk.LabelFrame(root, text="Enter Features")
features_frame.grid(row=0, column=0, padx=10, pady=10)

labels = [
    "Primary Type", "Location Description", "IUCR", "Domestic", "Ward",
    "X Coordinate", "Longitude", "Hour", "Month", "HourMinute",
    "FBI Code", "Year" , "day" , "Community Area"
]
entries = {}

for i, label in enumerate(labels):
    ttk.Label(features_frame, text=label).grid(row=i, column=0, padx=5, pady=2, sticky=tk.W)
    entry = ttk.Entry(features_frame)
    entry.grid(row=i, column=1, padx=5, pady=2)
    entries[label] = entry

entry_primary_type = entries["Primary Type"]
entry_location_description = entries["Location Description"]
entry_domestic = entries["Domestic"]
entry_ward = entries["Ward"]
entry_x_coord = entries["X Coordinate"]
entry_longitude = entries["Longitude"]
entry_hour = entries["Hour"]
entry_month = entries["Month"]
entry_hour_minute = entries["HourMinute"]
entry_IUCR = entries["IUCR"]
entry_FBI_Code = entries["FBI Code"]
entry_year = entries["Year"]
entry_Community_Area = entries["Community Area"]
entry_day = entries["day"]

# Model Selection
model_frame = ttk.LabelFrame(root, text="Select Model")
model_frame.grid(row=0, column=1, padx=10, pady=10)

ttk.Label(model_frame, text="Model").grid(row=0, column=0, padx=5, pady=5)
combo_model = ttk.Combobox(model_frame, state="readonly", width=30)
combo_model['values'] = [
    "Logistic Regression (Accuracy: 0.63)",
    "XGBoost (Accuracy: 0.9312)",
    "KNN (Accuracy: 0.91)",
    "Decision Tree (Accuracy: 0.9238)",
    "SVM (Accuracy: 0.626)",
    "LightGBM (Accuracy: 0.918)",
    "CatBoost (Accuracy: 0.9325)"
]
combo_model.grid(row=1, column=0, padx=5, pady=5)

# Predict Button
btn_predict = ttk.Button(root, text="Predict Arrest", command=predict_arrest)
btn_predict.grid(row=1, column=0, columnspan=2, pady=10)

root.mainloop()
